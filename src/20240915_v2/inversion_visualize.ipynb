{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms as tfms\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful function for later\n",
    "def load_image(url, size=None):\n",
    "    response = requests.get(url, timeout=0.2)\n",
    "    img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    if size is not None:\n",
    "        img = img.resize(size)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample function (regular DDIM)\n",
    "\n",
    "def sample(\n",
    "    prompt,\n",
    "    start_step=0,\n",
    "    start_latents=None,\n",
    "    guidance_scale=3.5,\n",
    "    num_inference_steps=30,\n",
    "    num_images_per_prompt=1,\n",
    "    do_classifier_free_guidance=True,\n",
    "    negative_prompt=\"\",\n",
    "    device=device,\n",
    "):\n",
    "\n",
    "    # Encode prompt\n",
    "    text_embeddings = pipe._encode_prompt(\n",
    "        prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt\n",
    "    )\n",
    "\n",
    "    # Set num inference steps\n",
    "    pipe.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "\n",
    "    # Create a random starting point if we don't have one already\n",
    "    if start_latents is None:\n",
    "        start_latents = torch.randn(1, 4, 64, 64, device=device)\n",
    "        start_latents *= pipe.scheduler.init_noise_sigma\n",
    "\n",
    "    latents = start_latents.clone()\n",
    "\n",
    "\n",
    "    # We'll keep a list of the inverted latents as the process goes on\n",
    "    intermediate_latents = []\n",
    "\n",
    "    for i in tqdm(range(start_step, num_inference_steps)):\n",
    "\n",
    "        t = pipe.scheduler.timesteps[i]\n",
    "\n",
    "        # Expand the latents if we are doing classifier free guidance\n",
    "        latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "        latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "        # Predict the noise residual\n",
    "        noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "        # Perform guidance\n",
    "        if do_classifier_free_guidance:\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "        # Normally we'd rely on the scheduler to handle the update step:\n",
    "        # latents = pipe.scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "        # Instead, let's do it ourselves:\n",
    "        prev_t = max(1, t.item() - (1000 // num_inference_steps))  # t-1\n",
    "        alpha_t = pipe.scheduler.alphas_cumprod[t.item()]\n",
    "        alpha_t_prev = pipe.scheduler.alphas_cumprod[prev_t]\n",
    "        predicted_x0 = (latents - (1 - alpha_t).sqrt() * noise_pred) / alpha_t.sqrt()\n",
    "        direction_pointing_to_xt = (1 - alpha_t_prev).sqrt() * noise_pred\n",
    "        latents = alpha_t_prev.sqrt() * predicted_x0 + direction_pointing_to_xt\n",
    "\n",
    "    # Post-processing\n",
    "    images = pipe.decode_latents(latents)\n",
    "    images = pipe.numpy_to_pil(images)\n",
    "\n",
    "    return images, intermediate_latents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inversion\n",
    "def invert(\n",
    "    start_latents,\n",
    "    prompt,\n",
    "    guidance_scale=3.5,\n",
    "    num_inference_steps=80,\n",
    "    num_images_per_prompt=1,\n",
    "    do_classifier_free_guidance=True,\n",
    "    negative_prompt=\"\",\n",
    "    device=device,\n",
    "):\n",
    "\n",
    "    # Encode prompt\n",
    "    text_embeddings = pipe._encode_prompt(\n",
    "        prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt\n",
    "    )\n",
    "\n",
    "    # Latents are now the specified start latents\n",
    "    latents = start_latents.clone()\n",
    "\n",
    "    # We'll keep a list of the inverted latents as the process goes on\n",
    "    intermediate_latents = []\n",
    "\n",
    "    # Set num inference steps\n",
    "    pipe.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "\n",
    "    # Reversed timesteps <<<<<<<<<<<<<<<<<<<<\n",
    "    timesteps = reversed(pipe.scheduler.timesteps)\n",
    "\n",
    "    for i in tqdm(range(1, num_inference_steps), total=num_inference_steps - 1):\n",
    "\n",
    "        # We'll skip the final iteration\n",
    "        if i >= num_inference_steps - 1:\n",
    "            continue\n",
    "\n",
    "        t = timesteps[i]\n",
    "\n",
    "        # Expand the latents if we are doing classifier free guidance\n",
    "        latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "        latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "        # Predict the noise residual\n",
    "        noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "        # Perform guidance\n",
    "        if do_classifier_free_guidance:\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "        current_t = max(0, t.item() - (1000 // num_inference_steps))  # t\n",
    "        next_t = t  # min(999, t.item() + (1000//num_inference_steps)) # t+1\n",
    "        alpha_t = pipe.scheduler.alphas_cumprod[current_t]\n",
    "        alpha_t_next = pipe.scheduler.alphas_cumprod[next_t]\n",
    "\n",
    "        # Inverted update step (re-arranging the update step to get x(t) (new latents) as a function of x(t-1) (current latents)\n",
    "        latents = (latents - (1 - alpha_t).sqrt() * noise_pred) * (alpha_t_next.sqrt() / alpha_t.sqrt()) + (\n",
    "            1 - alpha_t_next\n",
    "        ).sqrt() * noise_pred\n",
    "\n",
    "        # Store\n",
    "        intermediate_latents.append(latents)\n",
    "\n",
    "    return torch.cat(intermediate_latents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't connect to the Hub: 404 Client Error. (Request ID: Root=1-66e6ff56-5fa48a62638529ba1286655f;0caae47f-24fd-413d-96fb-005d07a53c4c)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/api/models/runwayml/stable-diffusion-v1-5.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated..\n",
      "Will try to load from local cache.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "125f8de7096d4d548196641824354980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DeferredCudaCallError",
     "evalue": "CUDA call failed lazily at initialization with error: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"../aten/src/ATen/cuda/CUDAContext.cpp\":50, please report a bug to PyTorch. \n\nCUDA call was originally invoked at:\n\n['  File \"<frozen runpy>\", line 198, in _run_module_as_main\\n', '  File \"<frozen runpy>\", line 88, in _run_code\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\\n    app.launch_new_instance()\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\\n    app.start()\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 736, in start\\n    self.io_loop.start()\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\\n    self.asyncio_loop.run_forever()\\n', '  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\\n    self._run_once()\\n', '  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\\n    handle._run()\\n', '  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\\n    self._context.run(self._callback, *self._args)\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\\n    await self.process_one()\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\\n    await dispatch(*args)\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\\n    await result\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\\n    reply_content = await reply_content\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\\n    res = shell.run_cell(\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\\n    return super().run_cell(*args, **kwargs)\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\\n    result = self._run_cell(\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\\n    result = runner(coro)\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\\n    coro.send(None)\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\\n    if await self.run_code(code, result, async_=asy):\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\\n    exec(code_obj, self.user_global_ns, self.user_ns)\\n', '  File \"/tmp/ipykernel_1949919/828364716.py\", line 1, in <module>\\n    import torch\\n', '  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\\n', '  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\\n', '  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\\n', '  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\\n', '  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/torch/__init__.py\", line 1146, in <module>\\n    _C._initExtension(manager_path())\\n', '  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\\n', '  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\\n', '  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\\n', '  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\\n', '  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 197, in <module>\\n    _lazy_call(_check_capability)\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 195, in _lazy_call\\n    _queued_calls.append((callable, traceback.format_stack()))\\n']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/torch/cuda/__init__.py:260\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m     \u001b[43mqueued_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/torch/cuda/__init__.py:145\u001b[0m, in \u001b[0;36m_check_capability\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[0;32m--> 145\u001b[0m     capability \u001b[38;5;241m=\u001b[39m \u001b[43mget_device_capability\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     major \u001b[38;5;241m=\u001b[39m capability[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/torch/cuda/__init__.py:381\u001b[0m, in \u001b[0;36mget_device_capability\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Gets the cuda capability of a device.\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;124;03m    tuple(int, int): the major and minor cuda capability of the device\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 381\u001b[0m prop \u001b[38;5;241m=\u001b[39m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prop\u001b[38;5;241m.\u001b[39mmajor, prop\u001b[38;5;241m.\u001b[39mminor\n",
      "File \u001b[0;32m/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/torch/cuda/__init__.py:399\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid device id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"../aten/src/ATen/cuda/CUDAContext.cpp\":50, please report a bug to PyTorch. ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDeferredCudaCallError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m MASTER_TYPE \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m----> 3\u001b[0m     pipe \u001b[38;5;241m=\u001b[39m \u001b[43mStableDiffusionPipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrunwayml/stable-diffusion-v1-5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMASTER_TYPE\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     pipe\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;241m=\u001b[39m DDIMScheduler\u001b[38;5;241m.\u001b[39mfrom_config(pipe\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#input_image = load_image(\"https://images.pexels.com/photos/8306128/pexels-photo-8306128.jpeg\", size=(512, 512))\u001b[39;00m\n",
      "File \u001b[0;32m/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/diffusers/pipelines/pipeline_utils.py:884\u001b[0m, in \u001b[0;36mDiffusionPipeline.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    880\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe module \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been loaded in 8bit and moving it to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m via `.to()` is not yet supported. Module is still on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    882\u001b[0m     )\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 884\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    887\u001b[0m     module\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(device) \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m silence_dtype_warnings\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_offloaded\n\u001b[1;32m    891\u001b[0m ):\n\u001b[1;32m    892\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    893\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    894\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not recommended to move them to `cpu` as running them will fail. Please make\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `torch_dtype=torch.float16` argument, or use another device for inference.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    898\u001b[0m     )\n",
      "File \u001b[0;32m/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/transformers/modeling_utils.py:2724\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2719\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2720\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2721\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2722\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2723\u001b[0m         )\n\u001b[0;32m-> 2724\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 797 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/torch/cuda/__init__.py:264\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    262\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA call failed lazily at initialization with error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    263\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA call was originally invoked at:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00morig_traceback\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 264\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m DeferredCudaCallError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28mdelattr\u001b[39m(_tls, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_initializing\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mDeferredCudaCallError\u001b[0m: CUDA call failed lazily at initialization with error: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED at \"../aten/src/ATen/cuda/CUDAContext.cpp\":50, please report a bug to PyTorch. \n\nCUDA call was originally invoked at:\n\n['  File \"<frozen runpy>\", line 198, in _run_module_as_main\\n', '  File \"<frozen runpy>\", line 88, in _run_code\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\\n    app.launch_new_instance()\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\\n    app.start()\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 736, in start\\n    self.io_loop.start()\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\\n    self.asyncio_loop.run_forever()\\n', '  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\\n    self._run_once()\\n', '  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\\n    handle._run()\\n', '  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\\n    self._context.run(self._callback, *self._args)\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\\n    await self.process_one()\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\\n    await dispatch(*args)\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\\n    await result\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\\n    reply_content = await reply_content\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\\n    res = shell.run_cell(\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\\n    return super().run_cell(*args, **kwargs)\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\\n    result = self._run_cell(\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\\n    result = runner(coro)\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\\n    coro.send(None)\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\\n    if await self.run_code(code, result, async_=asy):\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\\n    exec(code_obj, self.user_global_ns, self.user_ns)\\n', '  File \"/tmp/ipykernel_1949919/828364716.py\", line 1, in <module>\\n    import torch\\n', '  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\\n', '  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\\n', '  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\\n', '  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\\n', '  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/torch/__init__.py\", line 1146, in <module>\\n    _C._initExtension(manager_path())\\n', '  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\\n', '  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\\n', '  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\\n', '  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\\n', '  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 197, in <module>\\n    _lazy_call(_check_capability)\\n', '  File \"/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 195, in _lazy_call\\n    _queued_calls.append((callable, traceback.format_stack()))\\n']"
     ]
    }
   ],
   "source": [
    "MASTER_TYPE = torch.float16\n",
    "with torch.inference_mode():\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=MASTER_TYPE).to(device)\n",
    "\n",
    "    pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "    #input_image = load_image(\"https://images.pexels.com/photos/8306128/pexels-photo-8306128.jpeg\", size=(512, 512))\n",
    "    input_image = Image.open(\"input_dog.jpeg\").resize((512, 512))\n",
    "\n",
    "    input_image_prompt = \"Photograph of a puppy on the grass\"\n",
    "\n",
    "    # Encode with VAE\n",
    "    with torch.no_grad():\n",
    "        latent = pipe.vae.encode((tfms.functional.to_tensor(input_image).unsqueeze(0).to(device)* 2 - 1).half())\n",
    "    latent = 0.18215 * latent.latent_dist.sample()\n",
    "\n",
    "    inverted_latents = invert(latent, input_image_prompt, num_inference_steps=50)\n",
    "    inverted_latents.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pytorch2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
