{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_FRAME = 60\n",
    "SCENES = ['14n_copyroom10', '14n_office14', 'everett_dining1', 'everett_kitchen4','everett_kitchen6', 'everett_kitchen8']\n",
    "GUIDANCE_SCALES = ['1.0','3.0','5.0','7.0']\n",
    "# METHODS = ['shcoeffs']\n",
    "# CONTROLS = ['no_control','depth','normal','both','bae', 'bae_both']\n",
    "# NUM_CONTROL = [1,1,1,2,1,2]\n",
    "METHODS = ['vae']\n",
    "CONTROLS = ['no_control','depth','bae','bae_both']\n",
    "NUM_CONTROL = [1,1,1,2]\n",
    "\n",
    "LR = '1e-4'\n",
    "INPUT_DIR = \"../../output/20240918/val_multillum_val_rotate/\"\n",
    "OUTPUT_DIR = \"../../output/20240918/val_multillum_val_rotate_video_frame\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597ce5956db14b2abfc34b3f34d9edcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vll/venv_pytorch2.0/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for frame_id in tqdm(range(TOTAL_FRAME)):\n",
    "    for method in METHODS:\n",
    "        for guidance_scale in GUIDANCE_SCALES:\n",
    "            images = []\n",
    "            output_dir = os.path.join(OUTPUT_DIR, method, guidance_scale)\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            # first row is environment map\n",
    "            images.append(torch.zeros(3,256,256))\n",
    "            images.append(torch.zeros(3,256,256))\n",
    "            for scene in SCENES:\n",
    "                try:\n",
    "                    env_path = f'/data/pakkapon/datasets/multi_illumination/spherical/val_rotate/env_ldr/{scene}/dir_{frame_id}_mip2.png'\n",
    "                    image = torchvision.io.read_image(env_path) / 255.0\n",
    "                    images.append(image)\n",
    "                except:\n",
    "                    images.append(torch.zeros(3,256,256))\n",
    "            for control, num_control in zip(CONTROLS, NUM_CONTROL):\n",
    "                input_guidance_dir = os.path.join(INPUT_DIR, method, guidance_scale, control, LR)\n",
    "                # get lastest checkpoint \n",
    "                try:\n",
    "                    lastest_checkpoint = sorted(os.listdir(input_guidance_dir))[-1]\n",
    "                except:\n",
    "                    lastest_checkpoint = 'chk0'\n",
    "                input_dir = os.path.join(input_guidance_dir, lastest_checkpoint, 'lightning_logs', 'version_0')\n",
    "                filename_template = \"{scene}-dir_0_mip2_{scene}-dir_{frame_id}_mip2.jpg\"\n",
    "                filename = filename_template.format(scene=SCENES[0], frame_id=frame_id)\n",
    "                if num_control == 1:\n",
    "                    images.append(torch.zeros(3,256,256))\n",
    "                    # read control_image as tensor size (3,256,256)\n",
    "                    try:\n",
    "                        control_path = os.path.join(input_dir,'control_image', filename)\n",
    "                        image = torchvision.io.read_image(control_path) / 255.0\n",
    "                        # resize image to 256x256\n",
    "                        image = torchvision.transforms.functional.resize(image, (256,256))\n",
    "                        images.append(image)\n",
    "                    except:\n",
    "                        images.append(torch.zeros(3,256,256))\n",
    "                else:                \n",
    "                    for control_id in range(num_control):\n",
    "                        try:\n",
    "                            # read control_image as tensor size (3,256,256)\n",
    "                            control_path = os.path.join(input_dir,'control_image', filename.replace('.jpg',f'_{control_id}.jpg'))\n",
    "                            #print(control_path)\n",
    "                            image = torchvision.io.read_image(control_path) / 255.0\n",
    "                            # resize image to 256x256\n",
    "                            image = torchvision.transforms.functional.resize(image, (256,256))\n",
    "                            images.append(image)\n",
    "                        except:\n",
    "                            images.append(torch.zeros(3,256,256))\n",
    "                for scene in SCENES:\n",
    "                    try:\n",
    "                        filename = filename_template.format(scene=scene, frame_id=frame_id)\n",
    "                        image = torchvision.io.read_image(os.path.join(input_dir,'crop_image', filename)) / 255.0\n",
    "                        # resize image to 256x256\n",
    "                        image = torchvision.transforms.functional.resize(image, (256,256))\n",
    "                        images.append(image)\n",
    "                    except:\n",
    "                        images.append(torch.zeros(3,256,256))\n",
    "            # make grid\n",
    "            grid = torchvision.utils.make_grid(images, nrow=8)\n",
    "            # save image\n",
    "            output_path = os.path.join(output_dir, f'{frame_id:04d}.png')\n",
    "            torchvision.utils.save_image(grid, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pytorch2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
